{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:11:33.876488100Z",
     "start_time": "2024-05-17T09:11:33.472718100Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "from math import sqrt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.metrics import accuracy_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c5ec5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Ensure necessary NLTK resources are downloaded\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('reviews_mixed.csv')\n",
    "\n",
    "# Define lemmatization function\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.lower().split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply lemmatization to the dataset\n",
    "df['Text_Lemmatized'] = df['Text'].apply(lemmatize_text)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "training_input, validation_input, training_output, validation_output = train_test_split(\n",
    "    df['Text_Lemmatized'], df['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform the text data to feature vectors\n",
    "training_embeddings = vectorizer.fit_transform(training_input).toarray()\n",
    "validation_embeddings = vectorizer.transform(validation_input).toarray()\n",
    "\n",
    "# Example text for prediction\n",
    "text = [\"By choosing a bike over a car, I'm reducing my environmental footprint. Cycling promotes eco-friendly transportation, and I'm proud to be part of that movement.\"]\n",
    "text_lemmatized = [lemmatize_text(t) for t in text]\n",
    "text_embedding = vectorizer.transform(text_lemmatized).toarray()\n",
    "\n",
    "# print(\"Training Embeddings Shape:\", training_embeddings.shape)\n",
    "# print(\"Validation Embeddings Shape:\", validation_embeddings.shape)\n",
    "# print(\"Text Embedding Shape:\", text_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a60eed7cf54225f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:11:34.005834200Z",
     "start_time": "2024-05-17T09:11:33.489240500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviews_mixed.csv')\n",
    "\n",
    "def get_training_and_validation_datas(df: pd.DataFrame, training_size = 0.8):\n",
    "    data_size = df.shape[0]\n",
    "    indexes = [i for i in range(data_size)]\n",
    "    training_index = np.random.choice(indexes,int(data_size*training_size))\n",
    "    validation_index = [i for i in range(data_size) if not i in training_index]\n",
    "    training_input = [df['Text'].iloc[index] for index in training_index]\n",
    "    training_output = [df['Sentiment'].iloc[index] for index in training_index]\n",
    "    validation_input = [df['Text'].iloc[index] for index in validation_index]\n",
    "    validation_output = [df['Sentiment'].iloc[index] for index in validation_index]\n",
    "    return training_input, training_output, validation_input, validation_output\n",
    "\n",
    "training_input,training_output,validation_input, validation_output = get_training_and_validation_datas(df)\n",
    "\n",
    "#bag of words:\n",
    "vectorizer = CountVectorizer()\n",
    "#tf-idf:\n",
    "# vectorizer = TfidfVectorizer(max_features=50)\n",
    "\n",
    "\n",
    "text = [\"By choosing a bike over a car, I'm reducing my environmental footprint. Cycling promotes eco-friendly transportation, and I'm proud to be part of that movement.\"]\n",
    "# training_embeddings = vectorizer.fit_transform(training_input).toarray()\n",
    "# validation_embeddings = vectorizer.transform(validation_input).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b214d1-dcd5-46a4-9026-ecd7df77c607",
   "metadata": {},
   "source": [
    "## KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "68c2099b75fbafe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:11:34.090879700Z",
     "start_time": "2024-05-17T09:11:33.551845200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47.61904761904761%\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=2, n_init=10)\n",
    "kmeans.fit(training_embeddings)\n",
    "\n",
    "label_names = [name for name in set(training_output)]\n",
    "validation_indexes = kmeans.predict(validation_embeddings)\n",
    "computed_outputs = [label_names[value] for value in validation_indexes]\n",
    "\n",
    "accuracy = accuracy_score(validation_output, computed_outputs)\n",
    "print(f'Accuracy: {accuracy*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8e4fa32de58e4444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:11:34.157566800Z",
     "start_time": "2024-05-17T09:11:33.982454700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: positive\n"
     ]
    }
   ],
   "source": [
    "text = [\"By choosing a bike over a car, I'm reducing my environmental footprint. Cycling promotes eco-friendly transportation, and I'm proud to be part of that movement.\"]\n",
    "input =  vectorizer.transform(text).toarray()\n",
    "label = kmeans.predict(input)\n",
    "print('Label:', label_names[label[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc5ba7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.38095238095239%\n",
      "Label: negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('reviews_mixed.csv')\n",
    "\n",
    "def get_training_and_validation_datas(df: pd.DataFrame, training_size=0.8):\n",
    "    data_size = df.shape[0]\n",
    "    indexes = np.arange(data_size)\n",
    "    training_index = np.random.choice(indexes, int(data_size * training_size), replace=False)\n",
    "    validation_index = np.setdiff1d(indexes, training_index)\n",
    "    \n",
    "    training_input = df['Text'].iloc[training_index].tolist()\n",
    "    training_output = df['Sentiment'].iloc[training_index].tolist()\n",
    "    validation_input = df['Text'].iloc[validation_index].tolist()\n",
    "    validation_output = df['Sentiment'].iloc[validation_index].tolist()\n",
    "    \n",
    "    return training_input, training_output, validation_input, validation_output\n",
    "\n",
    "training_input, training_output, validation_input, validation_output = get_training_and_validation_datas(df)\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_mean_embeddings(texts):\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "    # Take the mean of the token embeddings to create a single vector per sentence\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Get mean BERT embeddings for training and validation data\n",
    "training_embeddings = get_mean_embeddings(training_input)\n",
    "validation_embeddings = get_mean_embeddings(validation_input)\n",
    "\n",
    "# Initialize and fit KMeans\n",
    "kmeans = KMeans(n_clusters=2, n_init=10)\n",
    "kmeans.fit(training_embeddings)\n",
    "\n",
    "# Predict clusters for validation data\n",
    "validation_indexes = kmeans.predict(validation_embeddings)\n",
    "label_names = list(set(training_output))\n",
    "\n",
    "# Map the cluster labels to the actual sentiment labels\n",
    "computed_outputs = [label_names[idx] for idx in validation_indexes]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(validation_output, computed_outputs)\n",
    "print(f'Accuracy: {accuracy * 100}%')\n",
    "\n",
    "# Predict the cluster label for new text input\n",
    "new_text = [\"By choosing a bike over a car, I'm reducing my environmental footprint. Cycling promotes eco-friendly transportation, and I'm proud to be part of that movement.\"]\n",
    "new_input_embeddings = get_mean_embeddings(new_text)\n",
    "label = kmeans.predict(new_input_embeddings)\n",
    "print('Label:', label_names[label[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb683cd1a2cab0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1791a723f5bfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:11:34.174383600Z",
     "start_time": "2024-05-17T09:11:34.025203500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 35.78947368421053%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agglomerative = AgglomerativeClustering(n_clusters=2)\n",
    "agglomerative.fit(training_embeddings)\n",
    "\n",
    "label_names = [name for name in set(training_output)]\n",
    "validation_indexes = agglomerative.fit_predict(validation_embeddings)\n",
    "computed_outputs = [label_names[value] for value in validation_indexes]\n",
    "\n",
    "accuracy = accuracy_score(validation_output, computed_outputs)\n",
    "print(f'Accuracy: {accuracy * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a01f1f45957de",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7ce0e129a289b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:11:34.693301800Z",
     "start_time": "2024-05-17T09:11:34.113645300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.26315789473684%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "gmm.fit(training_embeddings)\n",
    "\n",
    "label_names = [name for name in set(training_output)]\n",
    "validation_indexes = gmm.predict(validation_embeddings)\n",
    "computed_outputs = [label_names[value] for value in validation_indexes]\n",
    "\n",
    "accuracy = accuracy_score(validation_output, computed_outputs)\n",
    "print(f'Accuracy: {accuracy * 100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
